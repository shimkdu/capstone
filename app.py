import streamlit as st
from langchain_core.documents import Document
from langchain_core.runnables import Runnable
from typing import List, Dict
from agent import run_graph, EvaluationVerdict # run_graphë¥¼ ì§ì ‘ í˜¸ì¶œ

# ë´‡ ë§Œë“¤ê¸°
st.set_page_config(page_title="ğŸ•µï¸ FakeNews", page_icon="ğŸ›¡ï¸", layout="wide")

# --- í—¤ë” ì„¹ì…˜ ---
st.title("ğŸ•µï¸ FakeNews: AI ê¸°ë°˜ íŒ©íŠ¸ì²´í¬")
st.markdown("""
    ë‰´ìŠ¤ ë‚´ìš©ì˜ ì‹ ë¢°ë„ë¥¼ **Gemini AI ì—ì´ì „íŠ¸**ê°€ ë¶„ì„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
    ğŸ”— **ë¶„ì„ì„ ì›í•˜ëŠ” ë‰´ìŠ¤ URL**ì„ ì•„ë˜ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.
""")

# --- ì…ë ¥ ì„¹ì…˜ ---
with st.container(border=True):
    query = st.text_input("ğŸ”— ë‰´ìŠ¤ URL ì…ë ¥", placeholder="ì˜ˆ: https://www.chosun.com/politics/2025/10/27/...")
    
    col1, col2, col3 = st.columns([1, 1, 1])
    
    if col2.button("ğŸ” ì‹ ë¢°ë„ í™•ì¸í•˜ê¸°", use_container_width=True, type="primary") and query.strip():
        
        if not (query.startswith("http://") or query.startswith("https://")):
            st.error("ğŸš¨ ìœ íš¨í•œ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì†Œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("â³ íŒ©íŠ¸ì²´í¬ ì—ì´ì „íŠ¸ê°€ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                result = run_graph(query) 
            except Exception as e:
                st.error(f"âŒ LangGraph ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}")
                st.exception(e)
                st.stop()


        if result is None or 'verdict' not in result:
            st.error("ğŸš¨ **ì‹œìŠ¤í…œ ì˜¤ë¥˜:** ë¶„ì„ ê²°ê³¼ ê°ì²´ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()

        # ì†Œìš” ì‹œê°„ì€ run_graphì—ì„œ ê³„ì‚°ëœ í›„ resultì— í¬í•¨ë˜ì–´ì•¼ í•¨ (í˜„ì¬ëŠ” N/A)
        st.success(f"âœ… ë¶„ì„ ì™„ë£Œ: AI í‰ê°€ ê²°ê³¼ì…ë‹ˆë‹¤.") 
        
        verdict: EvaluationVerdict = result['verdict']
        overall_score = verdict.overall_fake_probability
        
        # --- ìµœì¢… í‰ê°€ ë° ë©”íŠ¸ë¦­ìŠ¤ ì„¹ì…˜ ---
        st.header("ğŸ›¡ï¸ ìµœì¢… ì‹ ë¢°ë„ í‰ê°€")
        
        col_main, col_sub = st.columns([2, 1])

        with col_main:
            st.subheader("ì¢…í•© í—ˆìœ„ ê°€ëŠ¥ì„±")
            st.progress(overall_score)
            
            if overall_score >= 0.75:
                status_emoji = "ğŸš¨"
                st.error(f"{status_emoji} **{overall_score*100:.1f}% (ë†’ìŒ)**: ê°€ì§œë‰´ìŠ¤ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì •ë³´ í™•ì‚°ì„ ë©ˆì¶”ì„¸ìš”.")
            elif overall_score >= 0.45:
                status_emoji = "âš ï¸"
                st.warning(f"{status_emoji} **{overall_score*100:.1f}% (ë³´í†µ)**: ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ë¥¼ í†µí•´ ê²€ì¦í•˜ì„¸ìš”.")
            else:
                status_emoji = "ğŸŸ¢"
                st.info(f"{status_emoji} **{overall_score*100:.1f}% (ë‚®ìŒ)**: ì‹ ë¢°ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
            
            st.markdown(f"**ìµœì¢… íŒë‹¨ ìš”ì•½**")
            st.caption(verdict.final_judgment)


        with col_sub:
            st.metric("ê³¼ì¥ ì ìˆ˜ (0.0=ì§„ì‹¤)", f"{verdict.exaggeration_score:.2f}")
            st.metric("ì¶œì²˜ ë¶€ì¡± ì ìˆ˜ (0.0=ì¶©ë¶„)", f"{verdict.lack_of_sources_score:.2f}")
            st.metric("ë…¼ë¦¬ì  ì˜¤ë¥˜ ì ìˆ˜ (0.0=ë…¼ë¦¬ì )", f"{verdict.logical_errors_score:.2f}")

        st.divider()

        # --- ìƒì„¸ ë¶„ì„ ì„¹ì…˜ (UI ì •ë¦¬) ---
        st.header("ğŸ” ì—ì´ì „íŠ¸ ë¶„ì„ ìƒì„¸ ë‚´ì—­")
        
        # 1. íŒ©íŠ¸ì²´í¬ ìµœì¢… ê·¼ê±°
        with st.expander("ğŸ“ íŒ©íŠ¸ì²´í¬ ìµœì¢… ê²°ê³¼ ë° ê·¼ê±° ë³´ê¸°", expanded=True):
            st.markdown(result['fact_check'])

        # 2. ë¶„ì„ ê³¼ì • (ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ë‚¨ê¹€)
        with st.expander("ğŸ”¬ ë¶„ì„ ê³¼ì • (Analysis Flow)"): 
            st.subheader("â‘  ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡")
            st.write(result['search_queries'])

        # 3. ê²€ìƒ‰ ê²°ê³¼ ì¶œì²˜
        with st.expander("ğŸ“° ê²€ìƒ‰ ê²°ê³¼ ì¶œì²˜ (AIê°€ ê²€ì¦ì— ì‚¬ìš©í•œ ìë£Œ)"):
            if result['article_result']:
                for idx, article in enumerate(result['article_result']):
                    st.markdown(f"""
                        **{idx+1}. {article['title']}**
                        > *{article['summary']}*
                        
                        [ì›ë¬¸ ë³´ê¸°]({article['source_url']})
                        ---
                    """)
            else:
                st.info("ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í•˜ì—¬ ì™¸ë¶€ ê²€ì¦ ì—†ì´ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

        st.divider()
        
        st.subheader("ğŸ“Š í•­ëª©ë³„ ìƒì„¸ ì ìˆ˜ ë° ê·¼ê±°")
        
        st.markdown(f"**ê³¼ì¥ (Exaggeration): {verdict.exaggeration_score:.2f}**")
        st.caption(f"ê·¼ê±°: {verdict.exaggeration_reasoning}")
        
        st.markdown(f"**ì¶œì²˜ ë¶€ì¡± (Lack of sources): {verdict.lack_of_sources_score:.2f}**")
        st.caption(f"ê·¼ê±°: {verdict.lack_of_sources_reasoning}")
        
        st.markdown(f"**ë…¼ë¦¬ì  ì˜¤ë¥˜ (Logical errors): {verdict.logical_errors_score:.2f}**")
        st.caption(f"ê·¼ê±°: {verdict.logical_errors_reasoning}")
        
    else: 
        st.warning("ë‰´ìŠ¤ URLì„ ì…ë ¥í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")