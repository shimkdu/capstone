from langgraph.graph import StateGraph, END
from typing import TypedDict, List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate 
from langchain_core.output_parsers import StrOutputParser
from googlenewsdecoder import new_decoderv1
from gnews import GNews
from newspaper import Article
from urllib.parse import urlparse
from pydantic.v1 import BaseModel, Field 
import json 
import re 
import os 
import requests 

# --- Selenium ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# SSL ì¸ì¦ì„œ ê²€ì¦ ì˜¤ë¥˜ ìš°íšŒë¥¼ ìœ„í•œ requests ì„¤ì •
requests.packages.urllib3.disable_warnings()

# --- LLM ì„¤ì • (Gemini API ì‚¬ìš©, temperature=0.0) ---
MODEL_NAME = 'gemini-2.5-flash'
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.0, api_key=GEMINI_API_KEY)
llm_json = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.0, response_mime_type="application/json", api_key=GEMINI_API_KEY) 

# --- Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜ ---
class EvaluationVerdict(BaseModel):
    exaggeration_score: float = Field(..., description="ê³¼ì¥ ì ìˆ˜ (0.0=ì§„ì‹¤, 1.0=ê±°ì§“)")
    exaggeration_reasoning: str = Field(..., description="ê³¼ì¥ ì ìˆ˜ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±° (1-2ë¬¸ì¥)")
    lack_of_sources_score: float = Field(..., description="ì¶œì²˜ ë¶€ì¡± ì ìˆ˜ (0.0=ì§„ì‹¤, 1.0=ê±°ì§“)")
    lack_of_sources_reasoning: str = Field(..., description="ì¶œì²˜ ë¶€ì¡± ì ìˆ˜ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±° (1-2ë¬¸ì¥)")
    logical_errors_score: float = Field(..., description="ë…¼ë¦¬ì  ì˜¤ë¥˜ ì ìˆ˜ (0.0=ì§„ì‹¤, 1.0=ê±°ì§“)")
    logical_errors_reasoning: str = Field(..., description="ë…¼ë¦¬ì  ì˜¤ë¥˜ ì ìˆ˜ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±° (1-2ë¬¸ì¥)")
    overall_fake_probability: float = Field(..., description="ì „ì²´ í—ˆìœ„ ê°€ëŠ¥ì„± ì ìˆ˜ (0.0=ì§„ì‹¤, 1.0=ê±°ì§“)")
    final_judgment: str = Field(..., description="ì ìˆ˜ë¥¼ ì¢…í•©í•œ ìµœì¢… íŒë‹¨ ìš”ì•½ ë¬¸ì¥")


# --- State ì •ì˜ ---
class NewsState(TypedDict):
    input_type: str 
    input: str
    article_title: str
    article_text: str 
    article_result: List[dict]
    search_queries: List[str]
    keyword_summary: str
    fact_check_draft: str
    fact_check: str
    verdict: EvaluationVerdict 
    reference: str 

# --- 0. URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (â­ ë„¤ì´íŠ¸ ë‰´ìŠ¤(#article_body) ì¶”ê°€) ---
def extract_article_text(state: NewsState):
    print("\n[Node 0: extract_article_text] ğŸ•µï¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (Selenium)...")
    if state['input_type'] == 'text':
        print("...ì˜¤ë¥˜: URLë§Œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì…ë ¥ì„ ì°¨ë‹¨í•©ë‹ˆë‹¤.")
        state['article_text'] = ""
        state['keyword_summary'] = "ì¶”ì¶œëœ_ê¸°ì‚¬_ì—†ìŒ"
        state['fact_check'] = "URLì´ ì•„ë‹Œ í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state

    url = state['input']
    
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36")
    
    driver = None 
    try:
        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)
        driver.delete_all_cookies()
        driver.get(url)
        
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "meta[property='og:title']"))
        )
        title = driver.find_element(By.CSS_SELECTOR, "meta[property='og:title']").get_attribute('content')

        # â­ ë³¸ë¬¸ ì¶”ì¶œ (ë„¤ì´ë²„, ë‹¤ìŒ, ë„¤ì´íŠ¸ ìˆœìœ¼ë¡œ ì‹œë„)
        extracted_text = ""
        try:
            main_content = driver.find_element(By.CSS_SELECTOR, "#articleBodyContents")
            extracted_text = main_content.text
            print("...ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸(#articleBodyContents) í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ.")
        except Exception:
            try:
                main_content = driver.find_element(By.CSS_SELECTOR, "#dic_area")
                extracted_text = main_content.text
                print("...ë‹¤ìŒ ë‰´ìŠ¤ ë³¸ë¬¸(#dic_area) í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ.")
            except Exception:
                try:
                    # â­ ë„¤ì´íŠ¸ ë‰´ìŠ¤ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì¶”ê°€
                    main_content = driver.find_element(By.CSS_SELECTOR, "#article_body") 
                    extracted_text = main_content.text
                    print("...ë„¤ì´íŠ¸ ë‰´ìŠ¤ ë³¸ë¬¸(#article_body) í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ.")
                except Exception:
                    # ìœ„ ì„¸ ë°©ì‹ì´ ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ Newspaper3kë¡œ ìµœí›„ì˜ ì‹œë„
                    print("...íŠ¹ì • ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í•´ Newspaper3kë¡œ íŒŒì‹± ì‹œë„.")
                    html = driver.page_source
                    article = Article(url)
                    article.set_html(html) 
                    article.parse()
                    extracted_text = article.text

        if len(extracted_text) < 30: 
            raise ValueError("ì¶”ì¶œëœ ê¸°ì‚¬ ë³¸ë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë‚´ìš©ì´ ë¶€ì‹¤í•©ë‹ˆë‹¤.")
            
        state['article_title'] = title
        state['article_text'] = extracted_text 
        print(f"...ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ. (ì œëª©: {title})")
        
    except Exception as e:
        print(f"URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì—ëŸ¬ ë°œìƒ: {e}")
        state['article_title'] = ""
        state['article_text'] = "" 
        state['keyword_summary'] = "ì¶”ì¶œëœ_ê¸°ì‚¬_ì—†ìŒ"
        state['fact_check'] = "URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë¶€ì‹¤í•©ë‹ˆë‹¤. íŒ©íŠ¸ì²´í¬ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    finally:
        if driver:
            driver.quit() 
            
    return state


# --- 1. ì´ˆê¸° í‚¤ì›Œë“œ ì¶”ì¶œ ---
def extract_initial_keyword(state: NewsState):
    print("\n[Node 1: extract_initial_keyword] ğŸ§  Gemini APIë¡œ ì´ˆê¸° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    title = state['article_title']
    if not title or title == "" or state['keyword_summary'] == "ì¶”ì¶œëœ_ê¸°ì‚¬_ì—†ìŒ":
        print("...ì œëª©ì´ ì—†ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return state 
        
    prompt = ChatPromptTemplate([('system', 'ë‹¹ì‹ ì€ ì™¸ë¶€ ì§€ì‹ì„ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì˜¤ì§ ì…ë ¥ëœ í…ìŠ¤íŠ¸ "ê·¸ëŒ€ë¡œ" í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ê³„ì ì¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. í™˜ê°ì€ ì—„ê²©íˆ ê¸ˆì§€ë©ë‹ˆë‹¤.'),
    ('human', '''
        ì£¼ì–´ì§„ "ê¸°ì‚¬ ì œëª©:"ì—ì„œ **í•µì‹¬ ì¸ë¬¼, ì‚¬ê±´, ì¥ì†Œ**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ 2~3ê°œ ì¶”ì¶œí•˜ì„¸ìš”.

        **!!ì ˆëŒ€ì ì¸ ê·œì¹™!!:**
        1. **ì˜¤ì§ "ê¸°ì‚¬ ì œëª©:" ì•ˆì— ëª…ì‹œì ìœ¼ë¡œ "ì¡´ì¬í•˜ëŠ” ë‹¨ì–´"ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.**
        2. "ê¸°ì‚¬ ì œëª©:"ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ì ˆëŒ€ë¡œ ì—°ìƒí•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì—¬ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
        3. ìµœì¢… ì¶œë ¥ì€ ì¶”ì¶œëœ í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ í•œ ì¤„ë¡œ ì œê³µí•˜ì„¸ìš”.

        ê¸°ì‚¬ ì œëª©: {title}
    ''')])

    chain = prompt | llm | StrOutputParser()
    raw_query = chain.invoke({'title': title}).strip()
    
    initial_query = " ".join(raw_query.split()) 
    
    state['keyword_summary'] = initial_query
    state['search_queries'] = [initial_query]
    print(f"...ì¶”ì¶œëœ í‚¤ì›Œë“œ: {initial_query}")
    return state

# --- 2. ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìš”ì•½ ê³µí†µ ë¡œì§ (â­ ë„¤ì´íŠ¸ ë‰´ìŠ¤(#article_body) ì¶”ê°€) ---
def _search_and_summarize(state: NewsState):
    query = state['keyword_summary']
    if query == "ì¶”ì¶œëœ_ê¸°ì‚¬_ì—†ìŒ":
        return state
        
    def decode_url(url):
        interval_time = 5 
        try:
            decoded_url = new_decoderv1(url, interval=interval_time)
            return decoded_url["decoded_url"] if decoded_url.get("status") else None
        except Exception as e:
            print(f"URL ë””ì½”ë”© ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}") 
            return None

    print(f"...GNews APIë¡œ '{query}' ê²€ìƒ‰ ì¤‘...")
    google_news = GNews(language='ko', country='KR', max_results=3) 
    search_query = query.replace('+', ' ') 
    resp = google_news.get_news(search_query)

    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36")
    
    driver = None
    try:
        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)
    except Exception as e:
        print(f"ğŸš¨ í¬ë¡¬ ë“œë¼ì´ë²„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return state 

    article_list = []
    for item in resp:
        try:
            url = decode_url(item['url'])
            if not url: continue

            driver.delete_all_cookies()
            driver.get(url)
            
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "meta[property='og:title']"))
            )
            title = driver.find_element(By.CSS_SELECTOR, "meta[property='og:title']").get_attribute('content')
            
            # â­ ê²€ìƒ‰ëœ ê¸°ì‚¬ë“¤ë„ ë™ì¼í•˜ê²Œ ë³¸ë¬¸ ì˜ì—­ ì§ì ‘ ì§€ì •
            extracted_text = ""
            try:
                main_content = driver.find_element(By.CSS_SELECTOR, "#articleBodyContents")
                extracted_text = main_content.text
            except Exception:
                try:
                    main_content = driver.find_element(By.CSS_SELECTOR, "#dic_area")
                    extracted_text = main_content.text
                except Exception:
                    try:
                        # â­ ë„¤ì´íŠ¸ ë‰´ìŠ¤ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì¶”ê°€
                        main_content = driver.find_element(By.CSS_SELECTOR, "#article_body")
                        extracted_text = main_content.text
                    except Exception:
                        print(f"    - [{url}] íŠ¹ì • ì»¨í…Œì´ë„ˆ ì°¾ê¸° ì‹¤íŒ¨. Newspaper3k í´ë°± ì‚¬ìš©.")
                        html = driver.page_source
                        article = Article(url)
                        article.set_html(html)
                        article.parse()
                        extracted_text = article.text
            
            if len(extracted_text) > 50:
                print(f"...'{title}' ê¸°ì‚¬ ìš”ì•½ ì¤‘...")
                article_summary_prompt = ChatPromptTemplate([
                    ('system', 'ë‹¤ìŒ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.'),
                    ('human', 'ê¸°ì‚¬: {text}')
                ])
                summary_chain = article_summary_prompt | llm | StrOutputParser()
                summary = summary_chain.invoke({'text': extracted_text})
            else:
                summary = "ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±ìœ¼ë¡œ ìš”ì•½ ë¶ˆê°€."

            article_list.append({
                'title': title, 
                'summary': summary.strip(),
                'source_url': url
            })
        except Exception as e:
            print(f'ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}')
    
    if driver:
        driver.quit() 
        
    print(f"...ê²€ìƒ‰/ìš”ì•½ ì™„ë£Œ. ì´ {len(article_list)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬.")
    state['article_result'] = article_list
    return state

# 2-1. 1ì°¨ ë‰´ìŠ¤ ê²€ìƒ‰
def search_initial(state: NewsState):
    print(f"\n[Node 2: search_initial] ğŸ” 1ì°¨ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„ (ì¿¼ë¦¬: {state['keyword_summary']})...")
    return _search_and_summarize(state)


# --- 3. ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ì •ì œ ---
def refine_keyword(state: NewsState):
    print("\n[Node 3: refine_keyword] ğŸ”„ 1ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨. í‚¤ì›Œë“œ ì •ì œ ì‹œë„...")
    current_query = state['search_queries'][-1]
    
    prompt = ChatPromptTemplate([
        ('system', 'ë‹¹ì‹ ì€ ê²€ìƒ‰ ì‹¤íŒ¨ë¥¼ ë³µêµ¬í•˜ëŠ” ê²€ìƒ‰ì–´ ì •ì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì´ˆ ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ êµ¬ì²´ì ì´ì–´ì„œ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'),
        ('human', '''
            ìµœì´ˆ ì¿¼ë¦¬: "{current_query}"

            ì§€ì¹¨:
            1. ìœ„ ì¿¼ë¦¬ì—ì„œ **í•µì‹¬ ì‚¬ê±´(ì¸ë¬¼+í–‰ë™)**ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…í•˜ì„¸ìš”.
            2. ë„ˆë¬´ ì„¸ë¶€ì ì¸ ì¥ì†Œ, ë¸Œëœë“œ ì´ë¦„, ìˆ˜ì‹ì–´ ë“±ì€ **ì œê±°**í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆì„¸ìš”.
            3. **í•µì‹¬ ì‚¬ê±´**ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œì„¸ìš”.
            4. ì¶œë ¥ì€ ì •ì œëœ í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.

            ì˜ˆì‹œ 1:
            ìµœì´ˆ ì¿¼ë¦¬: "ì´ì‹œì˜ ë‘˜ì§¸ ì¶œì‚° 2ì£¼ 5ì²œë§Œì› ì¡°ë¦¬ì› ê¿ˆì˜ ì§‘ LGì „ì"
            ì •ì œëœ í‚¤ì›Œë“œ: "ì´ì‹œì˜ ì¶œì‚°"

            ì˜ˆì‹œ 2:
            ìµœì´ˆ ì¿¼ë¦¬: "ë„ë„ë“œ íŠ¸ëŸ¼í”„ ë¸”ë¼ë””ë¯¸ë¥´ í‘¸í‹´ ì •ìƒíšŒë‹´ ì•Œë˜ìŠ¤ì¹´"
            ì •ì œëœ í‚¤ì›Œë“œ: "íŠ¸ëŸ¼í”„ í‘¸í‹´ ì •ìƒíšŒë‹´"
        ''')
    ])
    
    chain = prompt | llm | StrOutputParser()
    raw_query = chain.invoke({'current_query': current_query}).strip()
    
    refined_query = " ".join(raw_query.split())
    
    state['keyword_summary'] = refined_query
    state['search_queries'].append(refined_query) 
    print(f"...ì •ì œëœ í‚¤ì›Œë“œ: {refined_query}")
    return state

# --- 4. 2ì°¨ ë‰´ìŠ¤ ê²€ìƒ‰ ---
def search_refined(state: NewsState):
    print(f"\n[Node 4: search_refined] ğŸ” 2ì°¨ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„ (ì¿¼ë¦¬: {state['keyword_summary']})...")
    return _search_and_summarize(state)


# --- 5. íŒ©íŠ¸ì²´í¬ ì´ˆì•ˆ ìƒì„± ---
def generate_draft(state: NewsState):
    print("\n[Node 5: generate_draft] ğŸ“ íŒ©íŠ¸ì²´í¬ ì´ˆì•ˆ ìƒì„± ì¤‘...")
    original_title = state['article_title']
    original_text = state['article_text']
    article_result = state['article_result']
    
    if not article_result:
        print("...ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ ì—†ì–´ 'íŒë‹¨ ë¶ˆê°€' ì´ˆì•ˆ ìƒì„±.")
        state['fact_check'] = f"**{state['search_queries']}** í‚¤ì›Œë“œë¡œ êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼, ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì—†ì´ëŠ” íŒ©íŠ¸ì²´í¬ íŒë‹¨ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì •ë³´ì˜ ì¶œì²˜ì™€ ì‹ ë¢°ë„ë¥¼ ì§ì ‘ í™•ì¸í•´ ë³´ì„¸ìš”."
        return state

    prompt = ChatPromptTemplate([
        ('system','ë‹¹ì‹ ì€ ì „ë¬¸ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ì‹¤ ì—¬ë¶€ íŒë‹¨ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.'),
        ('human', '''
            ë‹¤ìŒ 'ì›ë³¸ ê¸°ì‚¬'ì™€ 'ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼(ìš”ì•½)'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³  ìƒì„¸íˆ ì„œìˆ í•œ **ìµœì¢… ê²°ê³¼**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

            ì›ë³¸ ê¸°ì‚¬ ì œëª©: {original_title}
            ì›ë³¸ ê¸°ì‚¬ ë³¸ë¬¸: {original_text}
            
            ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼(ìš”ì•½): {article_result}

            ì§€ì¹¨:
            1. 'ì›ë³¸ ê¸°ì‚¬'ì˜ í•µì‹¬ ì£¼ì¥ì´ 'ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼'ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”.
            2. 'ì›ë³¸ ê¸°ì‚¬'ê°€ ì‚¬ì‹¤ì¸ì§€ ê±°ì§“ì¸ì§€ ìµœì¢… ê²°ë¡ ì„ ë‚´ë¦¬ì„¸ìš”.
    ''')])

    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({
        'original_title': original_title,
        'original_text': original_text,
        'article_result': article_result 
    })

    state['fact_check'] = result 
    print("...ìµœì¢… ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ.")
    return state


# --- 7. í‰ê°€ ---
def evaluate(state: NewsState):
    print("\n[Node 7: evaluate] âš–ï¸ ìµœì¢… í‰ê°€ ë° ì ìˆ˜ ì‚°ì¶œ ì¤‘ (JSON Mode)...")
    fact_result = state['fact_check']
    
    error_reasoning = "ë¶„ì„ ë¶ˆê°€ ë˜ëŠ” LLM ì˜¤ë¥˜ë¡œ ê·¼ê±° ìƒì„± ì‹¤íŒ¨"
    
    try:
        if "íŒë‹¨ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤." in fact_result:
            print("...íŒë‹¨ ë¶ˆê°€ ìƒíƒœë¡œ ìµœì¢… í‰ê°€.")
            state['verdict'] = EvaluationVerdict(
                exaggeration_score=0.5,
                exaggeration_reasoning="íŒë‹¨ ê·¼ê±°ê°€ ë¶€ì¡±í•˜ì—¬ ì ìˆ˜ë¥¼ 0.5ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.",
                lack_of_sources_score=1.0, 
                lack_of_sources_reasoning="ê²€ìƒ‰ëœ ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ì–´ ì¶œì²˜ ë¶€ì¡± ì ìˆ˜ë¥¼ 1.0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.",
                logical_errors_score=0.5,
                logical_errors_reasoning="íŒë‹¨ ê·¼ê±°ê°€ ë¶€ì¡±í•˜ì—¬ ì ìˆ˜ë¥¼ 0.5ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.",
                overall_fake_probability=0.7,
                final_judgment=fact_result
            )
            return state

        json_schema_str = EvaluationVerdict.schema_json(indent=2)
        escaped_json_schema_str = json_schema_str.replace('{', '{{').replace('}', '}}')

        prompt = ChatPromptTemplate([
            ('system', f'''ë‹¹ì‹ ì€ ê°€ì§œ ë‰´ìŠ¤ íƒì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ê³ , **ë°˜ë“œì‹œ** JSON í˜•ì‹ìœ¼ë¡œ ì ìˆ˜ë¥¼ ì¶œë ¥í•˜ì„¸ìš”. JSONì€ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ì™„ë²½í•˜ê²Œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.

    ìŠ¤í‚¤ë§ˆ:
    {escaped_json_schema_str}
    '''), 
            ('human', '''
        ë‹¤ìŒ íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ê³ , ê° í•­ëª© ì ìˆ˜ë¥¼ 0.0~1.0 ì‚¬ì´ë¡œ ë°°ì í•˜ì„¸ìš”.
        0ì ì— ê°€ê¹Œìš°ë©´ ì§„ì‹¤ì´ê³ , 1ì ì— ê°€ê¹Œìš°ë©´ ê±°ì§“ì…ë‹ˆë‹¤.

        íŒ©íŠ¸ì²´í¬ ê²°ê³¼: {fact_result}
        
        ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì— ëŒ€í•œ ì ìˆ˜ì™€ **ê° ì ìˆ˜ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±°(1-2ë¬¸ì¥)**ë¥¼ ì •í™•í•˜ê²Œ íŒë‹¨í•˜ê³ , ìµœì¢… íŒë‹¨ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.
        **ì£¼ì˜:** ì¶œë ¥ì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ê°ì²´ì—¬ì•¼ í•˜ë©°, ì–´ë–¤ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSON ê°ì²´ë§Œì„ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    ''')])
        
        chain = prompt | llm_json | StrOutputParser()
        
        json_string = chain.invoke({'fact_result': fact_result})
        
        if json_string.strip().startswith("```json"):
            json_string = json_string.strip()[7:-3].strip()
        
        result_dict = json.loads(json_string)
        state['verdict'] = EvaluationVerdict(**result_dict) 
        
        print("...JSON í‰ê°€ ë° ì ìˆ˜ ì‚°ì¶œ ì™„ë£Œ.")
        
    except Exception as e:
        print(f"JSON ì²˜ë¦¬/LLM í˜¸ì¶œ ìµœì¢… ì˜¤ë¥˜ ë°œìƒ: {e}")
        state['verdict'] = EvaluationVerdict(
            exaggeration_score=1.0, 
            exaggeration_reasoning=error_reasoning,
            lack_of_sources_score=1.0,
            lack_of_sources_reasoning=error_reasoning,
            logical_errors_score=1.0,
            logical_errors_reasoning=error_reasoning,
            overall_fake_probability=1.0, 
            final_judgment=f"LLM í˜¸ì¶œ ì‹¤íŒ¨ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {e.__class__.__name__}"
        )
            
    return state

# --- 8. ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ… ë¡œì§ ---
def route_on_search_result(state: NewsState):
    print("\n[Router] ğŸ§­ ê²€ìƒ‰ ê²°ê³¼ ë¼ìš°íŒ…...")
    if state['keyword_summary'] == "ì¶”ì¶œëœ_ê¸°ì‚¬_ì—†ìŒ":
        print("...ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨. í‰ê°€ë¡œ ì¦‰ì‹œ ì´ë™.")
        return "skip_all" 
    if state['article_result']:
        print("...1ì°¨ ê²€ìƒ‰ ì„±ê³µ. ì´ˆì•ˆ ìƒì„±ìœ¼ë¡œ ì´ë™.")
        return "search_success" 
    else:
        print("...1ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨. í‚¤ì›Œë“œ ì •ì œë¡œ ì´ë™.")
        return "search_fail" 

# --- Graph Build and Run ---
def run_graph(input_data: str):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì „ì²´ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    builder = StateGraph(NewsState)
    builder.add_node('extract_article_text', extract_article_text)
    builder.add_node('extract_initial_keyword', extract_initial_keyword)
    builder.add_node('search_initial', search_initial)
    builder.add_node('refine_keyword', refine_keyword)
    builder.add_node('search_refined', search_refined)
    builder.add_node('generate_draft', generate_draft)
    builder.add_node('evaluate', evaluate)

    builder.set_entry_point('extract_article_text') 
    builder.add_edge("extract_article_text", "extract_initial_keyword")
    builder.add_edge("extract_initial_keyword", "search_initial")
    
    builder.add_conditional_edges(
        "search_initial", 
        route_on_search_result, 
        {
            "search_success": "generate_draft",
            "search_fail": "refine_keyword",
            "skip_all": "evaluate" 
        }
    )
    
    builder.add_edge("refine_keyword", "search_refined")
    builder.add_edge("search_refined", "generate_draft")
    builder.add_edge("generate_draft", "evaluate")
    builder.add_edge("evaluate", END)

    graph = builder.compile()

    initial_state = NewsState(
        input_type='url',
        input=input_data,
        article_title="",
        article_text="",
        article_result=[],
        search_queries=[],
        keyword_summary="",
        fact_check="",
        verdict=None,
        reference="",
    ) 
    
    return graph.invoke(initial_state)